{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a0182e",
   "metadata": {},
   "source": [
    "# Main notebook for data processing in robo_romeo project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614dc0a",
   "metadata": {},
   "source": [
    "## Imports - this should do us for the whole project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8dbd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 16:59:33.084688: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 16:59:33.084823: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/tmp/ipykernel_7029/3341886489.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm().pandas()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f4b406d7504ea7bca55fd7494bdb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import string\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import Xception #to get pre-trained model Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #for text tokenization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense#Keras to build our CNN and LSTM\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dropout\n",
    "from tqdm import tqdm_notebook as tqdm #to check loop progress\n",
    "tqdm().pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32220b64",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c757e1",
   "metadata": {},
   "source": [
    " - load_doc( filename ) – To load the document file and read the contents of the file into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a62943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document file into memory\n",
    "def load_doc(filename):\n",
    "    # Open file to read\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f57d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124dab6",
   "metadata": {},
   "source": [
    " - load_descriptions(doc) – To create a description dictionary that will map images with all 5 captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d1071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        # create the list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8731b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n"
     ]
    }
   ],
   "source": [
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad247809",
   "metadata": {},
   "source": [
    " - clean_descriptions( descriptions) – to clean the data by taking all descriptions as input. This will perform several types of cleaning including uppercase to lowercase conversion, punctuation removal, and removal of the number containing words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846a2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4529474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            # remove punctuation from each token\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0372a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean descriptions\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b5488",
   "metadata": {},
   "source": [
    " - txt_vocab( descriptions ) – to create a vocabulary from all the unique words extracted out from descriptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5b34bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e583bbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d8d34",
   "metadata": {},
   "source": [
    " - save_descriptions( descriptions, filename ) – This function is used to store all the preprocessed descriptions into a file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "236707c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/agolovin/code/CMaxK/robo_romeo/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e15d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85206533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save descriptions\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e04f9f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/agolovin/code/CMaxK/robo_romeo/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4ddcca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "filename = '../Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "638b0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencing(file):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    #splitting the lines in to lists\n",
    "    \n",
    "    list = []\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    y = []\n",
    "    \n",
    "    list =[]\n",
    "    for i in lines:\n",
    "        list.append(i.split())\n",
    "    \n",
    "    for item in list:\n",
    "        item.insert(1,\"startsequence\")\n",
    "        item.append(\"endsequence\")\n",
    "    \n",
    "    # for appending X1,X2,y\n",
    "    for i in list:\n",
    "        for seq in range(2,len(i)):\n",
    "            X1.append(i[0])\n",
    "            X2.append(i[1:seq])\n",
    "            y.append(i[seq])  \n",
    "    \n",
    "    return(X1,X2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "923a4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,X2,y  = sequencing('descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38609ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37a8174f27b66aa48088b371987b6f5829d12ab78f74d93e45c348145d421904"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
